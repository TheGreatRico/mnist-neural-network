{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Algebra (Matrix and Vector Operations)\n",
    "\n",
    "Mathematical Analysis (Derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network architecture and Forward Propogation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture](architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer (Layer 0):\n",
    "\n",
    "$$A^{[0]} = X_{(dim: 784, m)}$$\n",
    "\n",
    "Array of original images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer (Layer 1):\n",
    "\n",
    "$$Z^{[1]}_{(10; m)} = W^{[1]}_{(10; 784)} A^{[0]}_{(784; m)} + bias^{[1]}_{(784; 1)}$$\n",
    "\n",
    "$$A^{[1]}_{(10; m)} = ActivationFunction(Z^{[1]}_{(10; m)}) = ReLU(Z^{[1]})$$\n",
    "Where:\n",
    "\n",
    "$W^{[1]}_{(10; 784)}$ - weights (connections between layers)\n",
    "\n",
    "$bias^{[1]}_{(10; m)}$ - constant added to shift the activation function (could also be interpreted as systematic error term)\n",
    "\n",
    "$Z^{[1]}_{(10; m)}$ - linear combination of weights with original image with added bias term\n",
    "\n",
    "$A^{[1]}_{(10; m)}$ - values of neurons after the Activation Function is applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectified Linear Unit (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU is an activation function for the first layer of our Neural Network. It is defined as positive part of its argument:\n",
    "$$\\begin{equation}\n",
    "  f(x)=max(0,x)=\\frac{x+|x|}{2}=\n",
    "    \\begin{cases}\n",
    "      x & \\text{if  $x>0$,}\\\\\n",
    "      0 & \\text{otherwise.}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"relu.png\" alt=\"RelU\" style=\"height: 200px; width:300px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer (Layer 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[2]}_{(10; m)} = W^{[2]}_{(10; 10)} A^{[1]}_{(10; m)} + bias^{[2]}_{(10; 1)}$$\n",
    "\n",
    "$$A^{[2]}_{(10; m)} = ActivationFunction(Z^{[2]}_{(10; m)}) = SoftMax(Z^{[2]})$$\n",
    "Where:\n",
    "\n",
    "$W^{[2]}_{(10; 10)}$ - weights (connections between layers)\n",
    "\n",
    "$bias^{[2]}_{(10; 1)}$ - constant added to shift the activation function (could also be interpreted as systematic error term)\n",
    "\n",
    "$Z^{[2]}_{(10; m)}$ - linear combination of weights with original image with added bias term\n",
    "\n",
    "$A^{[2]}_{(10; m)}$ - values of neurons after the Activation Function is applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sofmax is an activation function, that we use for the second layer of Neural Network. We can compute it using formula:\n",
    "$$\\frac{e^{z_{i}}}{\\sum_{K}^{j=1}e^{z_{j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"softmax_example.png\" alt=\"RelU\" style=\"height: 200px; width:400px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"softmax_plot.png\" alt=\"RelU\" style=\"height: 280px; width:400px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Technically we have one more step - take max value of the output vector from Softmax Function and it's index will be the predicted label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards Propogation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall network is a combination of function composition and matrix multiplication:\n",
    "$$ g(x) := f^{[L]} (W^{[L]} f^{[L-1]}(W^{[L-1]} ... f^{[1]}(W^{[1]}x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the predicted label $\\hat{y}$ we will go the opposite way (right to left on our picture). We will find how much prediction diviated from the actual $y$ label via cross entropy cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function is cross entropy loss, which is:\n",
    "\n",
    "$$ C(y, g(x))=-\\sum_{j}y_{j}\\ln (\\hat{y}_{j}) $$\n",
    "\n",
    "Let's also refresh what our actual label vectro looks like:\n",
    "\n",
    "$$ y = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] $$\n",
    "\n",
    "To explain what is  in cross entropy formula, we need to understand how exactly we get our predictons from the last layer. We pass throught the softmax all the predictions to get vector of probabilities, where each value is defined as:\n",
    "\n",
    "$$  \\hat{y}_{j} = g({x}_{j}) = g(Z^{[2]}) = A^{[2]} = p_{j}={\\frac{e^{z_{j}}}{\\sum_{k}e^{z_{k}}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the deviation from prediction we take combination of Cross Entropy and Softmax which will be our cost function. The added bonus is that it has really nice derivative (proof: https://www.youtube.com/watch?v=5-rVLSc2XdE&ab_channel=SmartAlphaAI):\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial Z^{[2]}} = \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}} =  A^{[2]} - y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize that function we will take it's *antigradient*.\n",
    "\n",
    "Gradient is defined as follows (derivatives we want to find are):\n",
    "$$\\nabla C = \\left( \\frac{\\partial C}{\\partial W^{[1]}}, \\frac{\\partial C}{\\partial (bias)^{[1]}}, \\frac{\\partial C}{\\partial W^{[2]}}, \\frac{\\partial C}{\\partial (bias)^{[1]}} \\right) $$\n",
    "\n",
    "Using the chain rule (цепное правило, или же производная сложной функции) we will find the actual derivative of $C$. To refresh we will write out the chain rule:\n",
    "\n",
    "Given\n",
    "$$ y = f(t) $$\n",
    "$$ x = g(t) $$\n",
    "\n",
    "The chain rule is:\n",
    "$$\\frac{\\partial y}{\\partial x} = \\frac{\\partial t}{\\partial x} \\cdot \\frac{\\partial y}{\\partial t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply the chain rule to our case:\n",
    "$$ Z^{[1]} = Z^{[1]} (W^{[1]}) $$\n",
    "$$ A^{[1]} = A^{[1]} (Z^{[1]}) $$\n",
    "$$ W^{[2]} = W^{[2]} (A^{[1]}) $$\n",
    "$$ Z^{[2]} = Z^{[2]} (W^{[2]}) $$\n",
    "$$ A^{[2]} = A^{[2]} (Z^{[2]}) $$\n",
    "$$ C = C(A^{[2]}) $$\n",
    "$$ C = C(A^{[2]} (Z^{[2]} (W^{[2]} (A^{[1]} (Z^{[1]} (W^{[1]})))))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the derivatives we need to compute are:\n",
    "$$\\frac{\\partial C}{\\partial W^{[1]}} = \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial C}{\\partial A^{[1]}} = \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial W^{[2]}}{\\partial A^{[1]}} \\cdot \\left( \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} \\cdot \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}} \\right) = \\left( \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial W^{[2]}}{\\partial A^{[1]}} \\right) \\cdot \\frac{\\partial C}{\\partial W^{[2]}} $$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{[1]}} = \\frac{\\partial Z^{[1]}}{\\partial b^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial C}{\\partial A^{[1]}} = \\frac{\\partial Z^{[1]}}{\\partial b^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial W^{[2]}}{\\partial A^{[1]}} \\cdot \\left( \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} \\cdot \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}} \\right) = \\left( \\frac{\\partial Z^{[1]}}{\\partial b^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial W^{[2]}}{\\partial A^{[1]}} \\right) \\cdot \\frac{\\partial C}{\\partial W^{[2]}}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^{[2]}} = \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} \\cdot \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{[2]}} = \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} \\cdot \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start from the bottom:\n",
    "$$ \\frac{\\partial C}{\\partial b^{[2]}} = \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} \\cdot \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}} = A^{[2]} - y $$\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial Z^{[2]}} = \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}} = A^{[2]} - y $$\n",
    "\n",
    "$$Z^{[2]}_{(10; m)} = W^{[2]}_{(10; 10)} A^{[1]}_{(10; m)} + bias^{[2]}_{(10; 1)}$$\n",
    "\n",
    "$$ \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} = 1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next deriv is:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^{[2]}} = \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} \\cdot \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}} = \\left( A^{[2]} - y\\right) \\cdot A^{[1]T}_{(10; m)}$$\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial Z^{[2]}} = \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\cdot \\frac{\\partial C}{\\partial A^{[2]}} =  A^{[2]} - y $$\n",
    "\n",
    "$$Z^{[2]}_{(10; m)} = W^{[2]}_{(10; 10)} A^{[1]}_{(10; m)} + bias^{[2]}_{(10; 1)}$$\n",
    "\n",
    "$$ \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} = \\frac{\\partial C}{\\partial Z^{[2]}} \\cdot A^{[1]T}_{(10; m)} = \\left( A^{[2]} - y\\right) \\cdot A^{[1]T}_{(10; m)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move on to the first layer:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{[1]}} = \\frac{\\partial Z^{[1]}}{\\partial b^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial C}{\\partial A^{[1]}} = W^{[2]T} \\left( A^{[2]} - y \\right) \\cdot ReLU_{Z^{[2]}}^{'}{(Z^{[2]})}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial Z^{[1]}} = W^{[2]T} \\frac{\\partial C}{\\partial Z^{[1]}} \\cdot ReLU_{Z^{[2]}}^{'}{(Z^{[2]})} = W^{[2]T} \\left( A^{[2]} - y \\right) \\cdot ReLU_{Z^{[2]}}^{'}{(Z^{[2]})}$$\n",
    "\n",
    "ReLU derivative is:\n",
    "$$A^{[1]}_{(10; m)} = ReLU(Z^{[1]}_{(10; m)})$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "  \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}}=\n",
    "    \\begin{cases}\n",
    "      Z^{[1]} & \\text{if $Z^{[1]}>0$,}\\\\\n",
    "      0 & \\text{if $Z^{[1]}<0$.}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last derivative of gradient is:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^{[1]}} = \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial C}{\\partial A^{[1]}} = \\left( W^{[2]T} \\left( A^{[2]} - y \\right) \\cdot ReLU_{Z^{[2]}}^{'}{(Z^{[2]})} \\right) X^{T} $$\n",
    "\n",
    "We've already computed all we need to find the derivative:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^{[1]}} = \\frac{\\partial C}{\\partial Z^{[1]}} X^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual derivatives are given as:\n",
    "\n",
    "$${\\partial Z^{[2]}_{(10; m)}} = A^{[2]}_{(10; m)} - {y}_{(10; 1)}$$\n",
    "\n",
    "$${\\partial W^{[2]}_{(10; m)}} = \\frac{1}{m}{\\partial Z^{[2]}_{(10; m)}} A^{[1]T}_{(m; 10)}$$\n",
    "\n",
    "$${\\partial (bias)^{[2]}_{(10; 1)}} = \\frac{1}{m} \\sum_{1}^{10}{\\partial Z^{[2]}_{(10; m)}}$$\n",
    "\n",
    "$${\\partial Z^{[1]}_{(10; m)}} = W^{[2]T}_{(10; 10)} {\\partial Z^{[2]}_{(10; m)}} \\cdot ActivationFunction^{'}(Z^{[2]}_{(10; m)})$$\n",
    "\n",
    "$${\\partial W^{[1]}_{(10; m)}} = \\frac{1}{m}{\\partial Z^{[1]}_{(10; m)}} X^{T}_{(m; 784)}$$\n",
    "\n",
    "$${\\partial (bias)^{[1]}_{(10; 1)}} = \\frac{1}{m} \\sum_{1}^{10}{\\partial Z^{[1]}_{(10; m)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we update weights and bias with found values with hyperparameter Learning Rate to find set of weights and bias on the next iteration:\n",
    "\n",
    "$$ W^{[1]}_{(10; 784)} = W^{[1]}_{(10; 784)} - LearningRate \\cdot {\\partial W^{[1]}_{(10; m)}} $$\n",
    "\n",
    "$$ bias^{[1]}_{(10; m)} = bias^{[1]}_{(10; m)} - LearningRate \\cdot {\\partial (bias)^{[1]}_{(10; 1)}} $$\n",
    "\n",
    "$$ W^{[2]}_{(10; 10)} = W^{[2]}_{(10; 10)} - LearningRate \\cdot {\\partial W^{[2]}_{(10; m)}} $$\n",
    "\n",
    "$$ bias^{[2]}_{(10; 1)} = bias^{[2]}_{(10; 1)} - LearningRate \\cdot {\\partial (bias)^{[2]}_{(10; 1)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do now is run the Neural Network throughtout the whole dataset of 60k samples to train our model.\n",
    "\n",
    "To fully represent the model, it is necessary and sufficient to know all the parameters of weights and biases as well as the model architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
